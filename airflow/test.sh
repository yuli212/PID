#!/bin/bash

echo "ğŸ§ª Testing Airflow IoT ETL/ELT Pipelines"
echo "======================================="

# Test database connectivity
echo ""
echo "ğŸ”— Testing database connections..."
docker compose exec postgres psql -U airflow -d dwh -c "SELECT 'DWH Database Connected!' as status;" || exit 1

# Check DAG status via API
echo ""
echo "ğŸ“‹ Checking DAGs status..."
DAGS_RESPONSE=$(curl -s -u admin:admin http://localhost:8080/api/v1/dags)

if echo $DAGS_RESPONSE | grep -q "etl_iot_pipeline"; then
    echo "âœ… ETL Pipeline DAG found"
else
    echo "âŒ ETL Pipeline DAG not found"
fi

if echo $DAGS_RESPONSE | grep -q "elt_iot_pipeline"; then
    echo "âœ… ELT Pipeline DAG found"
else
    echo "âŒ ELT Pipeline DAG not found"
fi

# Test data files
echo ""
echo "ğŸ“Š Data files verification..."
echo "Sensors data sample:"
head -3 data/sensors.csv

echo ""
echo "Readings data sample:"
head -5 data/readings.csv

echo ""
echo "ğŸ¯ Ready for Demo!"
echo ""
echo "ğŸ“– Demo Steps:"
echo "1. Open http://localhost:8080"
echo "2. Login: admin / admin"
echo "3. Find and enable these DAGs:"
echo "   - etl_iot_pipeline (Transform in Memory)"
echo "   - elt_iot_pipeline (Transform in Database)" 
echo "4. Trigger both DAGs manually"
echo "5. Compare execution times and results"
echo ""
echo "ğŸ” After running DAGs, check results:"
echo "   docker compose exec postgres psql -U airflow -d dwh"
echo "   SELECT * FROM daily_sensor_summary_etl;"
echo "   SELECT * FROM daily_sensor_summary_elt;"
echo ""
echo "ğŸ Have fun exploring ETL vs ELT patterns!"